<html>
<head>
    <link href="./static/css/blog.css" rel='stylesheet' type='text/css'>
    <link href="./static/css/simple.min.css" rel='stylesheet' type='text/css'>
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
    <script src="./static/js/sh/sh_main.min.js"></script>
    <script src="./static/js/lazyload-min.js"></script>
    <script src="./static/js/hobo.js"></script>
    <title>Futex 0x3be6fec</title>
</head>
<body>
<div id="cont">
    <div class="blog-title"><a href="./index.html">Futex 0x3be6fec</a></div>
    <div class="top-link"><a href="./about.html">&#167; About</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="./archive.html">&#167; Archive</a></div>
    <div class="home-posts">
        <div class="preview">
        <div class="date">on Friday, July  5 &#39;13</div>
        <div class="title-home"><a href="./2013-07-05-Ring-balancing-for-the-weary.html">Ring balancing for the weary</a></div>

	<div class="post-wrapper">
	  <p>&ldquo;<strong>Ugh</strong>, Statistics!&rdquo; you say, &ldquo;I became a programmer in order to avoid this shit.&rdquo;  Me too, my friend, me too.  But there comes a time when you want to balance load between servers in a <a href="http://www.tom-e-white.com/2007/11/consistent-hashing.html">hash ring</a> and you end up wondering &ldquo;How many replicas of my servers do I need to be confident that my uniformly distributing hash function disperses those replicas evenly enough?&rdquo;  What you&rsquo;d like is simply a function that takes the size of your key space, the number of servers you want to store in the ring, and returns the number of replications per server to maintain a nearly uniform distribution.  Then you could move on with your life.</p>

<p>What does perfect look like?  If your servers hash to 32-bit integers, then your hash ring has 2<sup>32</sup> potential hashes.  Three servers uniformly distributed in a 32-bit space would be (2<sup>32</sup>)/3 32-bit integers apart.</p>

<p>&ldquo;When we add a new server, why don&rsquo;t we just move all of the existing servers along the ring a little to make room for the new one?&rdquo;  Sorry.  The point of using a consistent hash ring is to be able to remap as few data keys to new servers as possible when a server is added or deleted.  If we changed the hash position of every server in the ring, we&rsquo;d likely be remapping a lot of data keys to new servers.</p>

<blockquote><p><em>Constraint.</em> Move as few servers on the hash ring as possible.</p></blockquote>

<p><strong>Lazy.</strong>  What&rsquo;s the easiest way to be certain you have a uniform distribution over a space?  Fill ALL the spaces!  Imagine you&rsquo;re using a 16-bit hash space.  This isn&rsquo;t crazy, since you&rsquo;re only storing your servers in your hash ring usually.  A 16-bit hash space means 2<sup>16</sup> or 65536 potential hashes.  If you have no more than 100 servers, you could &ldquo;replicate&rdquo; &mdash; hash unique keys with your server name as the prefix &mdash; each server 650 times so that you would store 65500 keys in your ring.</p>

<blockquote><p><em>Collisions?</em>  Just rehash with an incrementing postfix until you find a hash that isn&rsquo;t taken.</p>

<p><em>Hash function?</em>  Use a fast one that produces uniformly distributed hashes and has good avalanche (collision resistance) behavior like the <a href="http://home.comcast.net/~bretm/hash/7.html">Jenkin&rsquo;s hash</a>.</p></blockquote>

<p>How uniform is this?  In the worst case, all 65500 are clumped together with a 65536-65500=36 gap.  That means 36/65536 of your ring is not uniformly distributed, only 0.05%.  Not bad.</p>

<p>What happens if you lose a machine?  You go from 100 machines to 99 machines, so you mark the 650 hashes in your ring corresponding to the dead sucker for removal.  Now you&rsquo;ve gone from 65500 hashes to 65500-650=64850.  Fewer machines means that you can have more replicas per machine.  If we round down 65536/99 we get 661 replications.  So we add 661-650=6 extra replicas per machine into the ring.  We&rsquo;re closer to 65536 now, with 65439 hashes in the ring.</p>

<p>What did that do to the likelihood that the ring is still just as uniformly distributed?  Worst case we have gap of size 65536-65439=97, so 97/65536 of the ring is not uniformly distributed, still very low at 0.14%.  As we approach zero machines, the likelihood that we&rsquo;ll be able to evenly divide our machine count into 65536 increases (perfect uniformity!), so the ring will stay nonuniform in the 0-1% range.  Also not bad.</p>

<p>&ldquo;Wait, I have to keep track of up to 65536 ordered hashes?&rdquo;  Yep.  You have to store all of those hashes and their metadata in memory somewhere.  For 65536 16-bit hashes, this isn&rsquo;t a big deal (around a megabyte with a string of IP metadata).  What if you&rsquo;re using 32-bit hashes, because you&rsquo;ve got significantly more than 100 machines?  That&rsquo;s 4294967296 32-bit hashes with metadata (around 50 gigabytes), which most modern laptops cannot fit in RAM.  If you&rsquo;re using a sorted data structure (useful for finding the &ldquo;next&rdquo; machine in the ring for a given data key), that means O(log n) for lookups or insertions.  That could also be quite annoying.</p>

<p><strong>Hopeful.</strong>  What if we were to find the largest gap in our ring and put our next hash right in the middle of this gap?  Throw the hash function out the window!  This is what would happen:</p>

<p><img src="./static/images/LargestGap.jpg" alt="Uniform" style="width: 300px;display:block; margin-left:auto; margin-right:auto;"/></p>

<p>As we add servers, the ring is only balanced when we have 2<sup>n</sup> servers, at 2,4,8,16,etc.  We&rsquo;ll never be able to have a perfectly balanced ring with three servers, for instance, or any odd number for that matter.  Removing servers is also a pain, because it&rsquo;s not intuitive how we would rebalance.  This is a hopeful approach, but don&rsquo;t take it.</p>

<p><strong>Empirical.</strong>  We can still hash servers into the ring, and fewer of them than the entire ring, if we know what degree of nonuniformity we can handle.  Empirically,</p>

<ol>
<li>Hash r replications of n servers into a s-bit space</li>
<li>Vary r by 1,10,100,500,1000,5000,10000 replications, so long as n*r is less than the number of hashes the space can fit</li>
<li>Vary s by 3,10,100,500,1000 servers, so long as n*r is also less than the size of the space</li>
<li>For each replica in the ring, find the number of data keys that would get routed to it</li>
<li>Sum up the number of data keys that would go to each server by reducing the replicas back to single servers</li>
<li>Get the difference between the number of data keys that would go to each server in a uniform world (2<sup>s</sup>)/n and the actual number of data keys per server</li>
<li>The sum of the differences in 6 gives us the nonuniformity of the entire ring</li>
</ol>


<p>Here are the results for a 16-bit space and 32-bit space (<a href="https://gist.github.com/edahlgren/5938401">code</a>):</p>

<div style="margin-left:auto; margin-right:auto;">
<div style="display:block; overflow:hidden; float:left; margin-left:25px; margin-right:50px;">
<div style="text-align:center;">16-bit</div>
<table>
<tr>
<td>servers</td>
<td>replicas</td>
<td>% nonuniform</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>30.415</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>19.422</td>
</tr>
<tr>
<td>3</td>
<td>100</td>
<td>6.370</td>
</tr>
<tr>
<td>3</td>
<td>500</td>
<td style="color:#0000CC;">0.724</td>
</tr>
<tr>
<td>3</td>
<td>1000</td>
<td>1.295</td>
</tr>
<tr>
<td>3</td>
<td>5000</td>
<td style="color:#0000CC;">0.593</td>
</tr>
<tr>
<td>3</td>
<td>10000</td>
<td style="color:#0000CC;">0.331</td>
</tr>
<tr></tr>
<tr>
<td>10</td>
<td>1</td>
<td>82.101</td>
</tr>
<tr>
<td>10</td>
<td>10</td>
<td>35.485</td>
</tr>
<tr>
<td>10</td>
<td>100</td>
<td>5.508</td>
</tr>
<tr>
<td>10</td>
<td>500</td>
<td>2.316</td>
</tr>
<tr>
<td>10</td>
<td>1000</td>
<td>2.249</td>
</tr>
<tr>
<td>10</td>
<td>5000</td>
<td style="color:#0000CC;">0.531</td>
</tr>
<tr></tr>
<tr>
<td>100</td>
<td>1</td>
<td>74.716</td>
</tr>
<tr>
<td>100</td>
<td>10</td>
<td>26.733</td>
</tr>
<tr>
<td>100</td>
<td>100</td>
<td>6.497</td>
</tr>
<tr>
<td>100</td>
<td>500</td>
<td>1.815</td>
</tr>
<tr></tr>
<tr>
<td>500</td>
<td>1</td>
<td>75.839</td>
</tr>
<tr>
<td>500</td>
<td>10</td>
<td>22.915</td>
</tr>
<tr>
<td>500</td>
<td>100</td>
<td>3.948</td>
</tr>
<tr></tr>
<tr>
<td>1000</td>
<td>1</td>
<td>70.220</td>
</tr>
<tr>
<td>1000</td>
<td>10</td>
<td>22.842</td>
</tr>
</table>
</div>

<div style="display:block; overflow:hidden; margin-left:50px; margin-right:auto;">
<div style="text-align:center;">32-bit</div>
<table>
<tr>
<td>servers</td>
<td>replicas</td>
<td>% nonuniform</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>30.415</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>19.418</td>
</tr>
<tr>
<td>3</td>
<td>100</td>
<td>6.379</td>
</tr>
<tr>
<td>3</td>
<td>500</td>
<td style="color:#0000CC;">0.660</td>
</tr>
<tr>
<td>3</td>
<td>1000</td>
<td>1.731</td>
</tr>
<tr>
<td>3</td>
<td>5000</td>
<td style="color:#0000CC;">0.647</td>
</tr>
<tr>
<td>3</td>
<td>10000</td>
<td style="color:#0000CC;">0.333</td>
</tr>
<tr></tr>
<tr>
<td>10</td>
<td>1</td>
<td>82.105</td>
</tr>
<tr>
<td>10</td>
<td>10</td>
<td>35.483</td>
</tr>
<tr>
<td>10</td>
<td>100</td>
<td>5.733</td>
</tr>
<tr>
<td>10</td>
<td>500</td>
<td>2.186</td>
</tr>
<tr>
<td>10</td>
<td>1000</td>
<td>2.476</td>
</tr>
<tr>
<td>10</td>
<td>5000</td>
<td>1.624</td>
</tr>
<tr>
<td>10</td>
<td>10000</td>
<td style="color:#0000CC;">0.694</td>
</tr>
<tr></tr>
<tr>
<td>100</td>
<td>1</td>
<td>74.740</td>
</tr>
<tr>
<td>100</td>
<td>10</td>
<td>27.578</td>
</tr>
<tr>
<td>100</td>
<td>100</td>
<td>7.369</td>
</tr>
<tr>
<td>100</td>
<td>500</td>
<td>2.938</td>
</tr>
<tr>
<td>100</td>
<td>1000</td>
<td>2.314</td>
</tr>
<tr>
<td>100</td>
<td>5000</td>
<td>1.233</td>
</tr>
<tr>
<td>100</td>
<td>10000</td>
<td style="color:#0000CC;">0.781</td>
</tr>
<tr></tr>
<tr>
<td>500</td>
<td>1</td>
<td>75.939</td>
</tr>
<tr>
<td>500</td>
<td>10</td>
<td>23.786</td>
</tr>
<tr>
<td>500</td>
<td>100</td>
<td>7.856</td>
</tr>
<tr>
<td>500</td>
<td>500</td>
<td>3.711</td>
</tr>
<tr>
<td>500</td>
<td>1000</td>
<td>2.598</td>
</tr>
</table>
</div>
</div>


<p>It was surprising to find that even 500 replicas of 3 servers in a 16-bit and 32-bit space could so readily result in a nearly (99%) uniform ring.  This test was only run once and there needs to be averaging over several runs, but the take-home is that you need far fewer keys than the full hash space to produce uniform rings at 99% uniformity.  Also more replicas with few servers is much better than more servers and few replicas.</p>

<blockquote><p><em>Ok, so how many replicas of my servers do I need?</em></p>

<p>Simple: Choose the smallest bit-space that supports at least 500 replications per server.<br></p>

<p>Better: Run the tests!  (and make the tests better!)</p></blockquote>
	</div>

    </div>
        <div class="preview">
        <div class="date">on Wednesday, July  3 &#39;13</div>
        <div class="title-home"><a href="./2013-07-03-Hash-functions-for-the-weary.html">Hash functions for the weary</a></div>

	<div class="post-wrapper">
	  <p>&ldquo;<strong>Damnit</strong>&rdquo;, you say.  &ldquo;Building this hash ring or routing this data means that I need to find the fastest hash function that isn&rsquo;t horrible.&rdquo;  The reason you&rsquo;ve said &ldquo;Damnit!&rdquo; is because what makes a hash function horrible is knowledge you&rsquo;ve popped off your memory stack long ago.  Simply put, a non-horrible hash function:</p>

<blockquote><p>Distributes your keys randomly (evenly) across your key space</p>

<p>Avoids collisions with other keys</p>

<p>Maybe is irreversible</p>

<p>Maybe compresses your key into a checksum</p>

<p>Is quick!</p></blockquote>

<p>What do you care about?  Maybe you care that all your machines have the same load, so you care that your keys hash to a uniform distribution.  Collisions don&rsquo;t matter to you because you&rsquo;re not storing anything about keys that hash to the same place.  Maybe you want to avoid iterating through a data structure when multiple values are stored at the same place, so you care about minimizing key collisions.  Maybe you care about garaunteeing that two messages aren&rsquo;t the same very quickly.  Maybe you care about how easy it would be to brute force find a collision for your hash.  Irreversable output (160 bit large) matters most to you.  But always, you care about &ldquo;Moar speed!&rdquo;</p>

<p>It&rsquo;s easy to evaulate a hash function for speed (clock those cpu seconds), or output size (derp de derp, how many bytes is this thing?).  Less obvious, a hash function produces a uniform distribution if:</p>

<blockquote><p>Each pattern of output bits is equally likely</p></blockquote>

<p>A hash function avoids collisions when:</p>

<blockquote><p>Changing a single input bit drastically affects output bits</p></blockquote>

<p>When you hear people talking about <em>avalanche</em>, this is what they mean.  Formally, &ldquo;drastically&rdquo; means that if an input bit changes, each output bit should change with a 50% probability.  When an input bit has a 0% or 100% affect on an output bit, we say that the output bit isn&rsquo;t <em>mixed</em>.  If this happens to a lot of your bits, then your hashing function <em>mixes poorly</em>.  Your hash function will also avoid collisions if:</p>

<blockquote><p>Output bits are independent of one another</p></blockquote>

<p>Those are the three properities you&rsquo;re looking for if you want your hash function to produce uniformly distributed and collision resistant output.  Now that we know what we&rsquo;re looking for and what we care about, let&rsquo;s look at some options:</p>

<p><strong>CRC32.</strong>  Standing for cyclic redundancy check, this simply produces a 32-bit &ldquo;tl;dr&rdquo; of your message (a <a href="http://en.wikipedia.org/wiki/Checksum">checksum</a>), useful to checking whether your message was transmitted with errors.  It&rsquo;s computed by continuously dividing a smaller and smaller slices of the input by a polynomial, eventually resulting in a 32-bit remainder [1].  <em>Pros.</em>  Since your polynomial is usually fixed, one can write very fast CRC implementations involving pre-computed lookup tables.  <em>Cons.</em>  A chi-squared test [2] shows that only half of the CRC32&rsquo;s output bits were uniformly distributed; each input bit affects each output bit at less than 33% probability or greater than 66% probability (we want 50%), meaning that it has abysmal collision resistance [2].</p>

<p><strong>FNV.</strong> Standing for its creators: (Glenn) Fowler/(Landon Cur) Noll/(Phong) Vo, FNV is a very simple hash function.  The idea is to take a starting seed, and continually multiply it by a specific prime and XOR it with the next byte of your input.  <em>Pros.</em>  The simplicity of the algorithm makes it fast enough to be used as a checksum.  <em>Cons.</em>  Its simplicity makes it trivial to brute force find collisions.  A chi-sqaured test shows that output bits are uniformly distributed up to only 2<sup>14</sup> [3].  It also appears that the last byte of the input does not cause any mixing at all [3], meaning that this function has poor collision resistance.</p>

<p><strong>SIPHASH.</strong> The hipster on the block, SipHash is a relatively new hash function (2012) which boasts to have better collision resistance than FNV (not unimaginable) and to be just as fast or faster. There are few benchmarks available, some of them only comparing Cycles/Byte (and only between SipHash and several <a href="https://en.wikipedia.org/wiki/Hash-based_message_authentication_code">hmac</a> implementations) [4], or language internal improvements over FNV [5].  I don&rsquo;t have enough information to recommend this function in terms of uniform distribution or collision resistance, but its speed looks comparable to FNV.</p>

<p><strong>JENKINS.</strong> Named after its creator Bob Jenkins, this hash function mixes keys 12 bytes at a time [6].  The mixing is a complicated sequence of shifts, adds, and XORs, making it non-trivial to implement.  <em>Pros.</em>  It produces uniformly distributed output, and has excellent collision resistance due to good mixing (every input bit causes a change in every output bit at 33-66% probability) [7].  Since the mixing can be done on parallel processors, it is still relatively fast.  <em>Cons.</em> It is not cryptographically secure and it is easy to implement incorrectly.</p>

<p><strong>SHA-1.</strong> Standing for Secure Hash Algorithm, this hash function produces cryptographic (hopefully irreversable) 160-bit output.  We can compare SHA-1 to our other algorithms (which can produce 32-bit output) by XOR-ing the five 32-bit sections of the 160-bit output together.  <em>Pros.</em>  Cryptographic hashes need to be uniformly distributed and they need to mix well, or else finding collisions by brute force would be possible.  SHA-1 has good mixing like Jenkins and produces output that is equally uniformly distributed [8].  It is also mostly cryptographically secure [9].  <em>Cons.</em>  Computing a 160-bit hash takes cpu seconds any way you slice it, so it&rsquo;s totally unnecessary if security isn&rsquo;t required.</p>

<p>Know what you want your hash function to do, and what you can compromise to get there!</p>

<blockquote><p>[1] <a href="http://en.wikipedia.org/wiki/Cyclic_redundancy_check">Cylic redundancy check: wikipedia</a><br>
[2] <a href="http://home.comcast.net/~bretm/hash/8.html">CRC32 test: Bret Mulvey</a><br>
[3] <a href="http://home.comcast.net/~bretm/hash/6.html">FNV test: Bret Mulvey</a><br>
[4] <a href="http://bench.cr.yp.to/results-auth.html">SipHash Bytes/Cycle benchmarks</a><br>
[5] <a href="http://www.serpentine.com/blog/2012/10/02/a-fast-new-siphash-implementation-in-haskell/">SipHash Haskell benchmarks</a><br>
[6] <a href="http://en.wikipedia.org/wiki/Jenkins_hash_function">Jenkins hash function: wikipedia</a><br>
[7] <a href="http://home.comcast.net/~bretm/hash/7.html">Jenkins test: Bret Mulvey</a><br>
[8] <a href="http://home.comcast.net/~bretm/hash/9.html">SHA-1 test: Bret Mulvey</a><br>
[9] <a href="http://www.schneier.com/blog/archives/2005/02/cryptanalysis_o.html">Bruce Schneier on SHA-1</a></p></blockquote>
	</div>

    </div>
        <div class="preview">
        <div class="date">on Sunday, June 23 &#39;13</div>
        <div class="title-home"><a href="./2013-06-23-Hobogen.html">Hobogen</a></div>

	<div class="post-wrapper">
	  <p><a href="http://github.com/edahlgren/hobo">Hobogen</a> is an html compiler written in haskell.  It is an extension of the minimalist blogging engine <a href="http://github.com/jamwt/hobo">hobo</a>.</p>

<p>Futex 0x3be6fec is generated by hobogen. I&rsquo;ve used it also to generate my <a href="prose.edahlgren.com">prose writings collection</a>.  Based on a set of <a href="http://mustache.github.io/">mustache</a> tags that you include in your html templates, and a posts that you write in simple (markdown)[http://whatismarkdown.com/], hobogen produces static html for:</p>

<ul>
<li>A home page, with a full preview of the post (like here), or an n-line preview of the post (like on the prose site)</li>
<li>A page for each of your posts</li>
<li>An archive page, where your posts are grouped by month</li>
<li>An about page where you can describe your site</li>
</ul>


<p>Hobogen builds custom headers and footers into generated html using head.html and foot.html templates.  These are good places to import css and javascript styles specific to your site.  Hobogen inherits from hobo the ability to highlight code blocks specific to a particular language. Here is a sample of the hobogen code that compiles the archive page:</p>

<pre><code>#!haskell
compileArchive ts cfg = do
    rpaths &lt;- getDirectoryContents (base cfg &lt;/&gt; "posts")
    let paths = (reverse . sort) rpaths
    let archives = catMaybes $ map parseArchive paths
    let groupedArchives =
      L.groupBy (\(y1,m1,_,_) (y2,m2,_,_) -&gt; y1 == y2 &amp;&amp; m1 == m2) archives
    (archiveTemplate ts) (sub groupedArchives)
</code></pre>

<p>In that code block, hobogen is reading all of the posts paths, sorting them in ascending dated order, parsing the parts of the posts for valid year, month, title and url data needed to construct the archive page (using the <a href="http://hackage.haskell.org/package/attoparsec-0.10.4.0">Attoparsec</a> parser), grouping the posts written in the same year and month, and building the html from the archive template.</p>

<p>If you&rsquo;d like to either to use or contribute to hobogen, contact <a href="https://github.com/edahlgren">me</a> through github.</p>
	</div>

    </div>
        </div>
</div>
</body>
</html>
