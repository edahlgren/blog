<html>
<head>
    <link href="./static/css/blog.css" rel='stylesheet' type='text/css'>
    <link href="./static/css/simple.min.css" rel='stylesheet' type='text/css'>
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
    <script src="./static/js/sh/sh_main.min.js"></script>
    <script src="./static/js/lazyload-min.js"></script>
    <script src="./static/js/hobo.js"></script>
    <title>Futex 0x3be6fec</title>
</head>
<body>
<div id="cont">
    <div class="blog-title"><a href="./index.html">Futex 0x3be6fec</a></div>
    <div class="top-link"><a href="./about.html">&#167; About</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="./archive.html">&#167; Archive</a></div>
    <div class="home-posts">
        <div class="preview">
        <div class="date">on Friday, July 19 &#39;13</div>
        <div class="title-home"><a href="./2013-07-19-Invisible-networks-for-the-sneaky.html">Invisible networks for the sneaky</a></div>

	<div class="post-wrapper">
	  <p>Eyebrows raise. Invisible networks you say &hellip;  If you type</p>

<pre><code> ifconfig
</code></pre>

<p>You&rsquo;ll see all of the network devices currently up: probably eth0 (ethernet) or wlan (wireless), and a lo (loopback) device at least (ifconfig -a for all devices up or down).  Little beknownst to you, these devices exist in a single <a href="http://lwn.net/Articles/219794/">network namespace</a>: a place for them all to romp and play happily together.  And by romp and play I mean they can be <a href="http://tldp.org/HOWTO/BRIDGE-STP-HOWTO/what-is-a-bridge.html">bridged together</a>, they can <a href="http://www.tldp.org/HOWTO/IP-Masquerade-HOWTO/ipmasq-background2.1.html">masquerade</a> as one another (I know, it&rsquo;s awesome), or deliberately forward each other packets.</p>

<p>What if I told you that you could steal one of these network devices and put it in a private, invisible network?  You might think, &ldquo;Hmm, that means I can probably run X program on P port in both networks and they won&rsquo;t conflict&rdquo;.  Correct.  Then you might say, &ldquo;Hey!  I could also hide everything I&rsquo;m doing in that invisible network from the rest of my machine&rdquo;.  Yep.  But after a few minutes you&rsquo;d probably say: &ldquo;What! How do I communicate with the outside world?&rdquo;  I&rsquo;d be worried too.</p>

<p>The answer: Virtual ethernet!</p>

<p>&ldquo;Wait, what?&rdquo;  Virtual ethernet devices are created in pairs, where one &ldquo;veth&rdquo; device can talk directly to its peer, no matter what network you move either of them to.  So if veth0 is in network A and veth1 is in network B, but network B is totally invisible to A, these networks can tunnel messages to one another over their veth secret chat line.  A stranger doesn&rsquo;t know anything about your private network topology other than a veth device exists.  Pretty cool.</p>

<p>&ldquo;Ok fine, how do I make an invisible network?&rdquo;  By making another network namespace, by making a pair of veth devices that can talk to each other, and by placing one in the new network namespace and the other in your current namespace.  Command line setup involves:</p>

<ul>
<li><a href="http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=ip-netns&amp;ampsect=8">ip netns</a>, for creating, removing, and running commands in network namespaces</li>
<li><a href="http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=ip-link&amp;sect=&amp;querytype=Man+page&amp;srch=&amp;case=sensitive">ip link</a>, for generally managing network devices</li>
<li><a href="http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=ip&amp;sect=addr&amp;querytype=Man+page&amp;srch=&amp;case=sensitive">ip addr</a>, for assigning inet and subnet addresses to network devices</li>
<li><a href="http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=ifconfig&amp;sect=addr&amp;querytype=Man+page&amp;srch=&amp;case=sensitive">ifconfig</a>, for checking the network configuration</li>
<li><a href="http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=nc&amp;sect=&amp;querytype=Man+page&amp;srch=&amp;case=sensitive">netcat</a>, for testing</li>
</ul>


<p>Warning: some of these commands will fail with mysterious error messages if you&rsquo;re not root.  <code>sudo su</code> to save yourself the confusion.</p>

<p>In Terminal 1:</p>

<pre><code>#!sh
# name a new network namespace
# this is not the network namespace you're in right now
# but we can enter it by name now at any time
ip netns add ns1

# check that the new network namespace is present
ip netns list

# create a pair of veth devices
ip link add host type veth peer name guest

# make sure you can see both host and guest (down)
ifconfig -a

# bring the host veth device up, it will exist in the current
# network namespace
ip link set host up

# give up the guest veth device to the new network
ip link set guest netns ns1

# assign an ip and subnet to the host veth
ip addr add 192.168.0.101/24 dev host

# check that the address shows up
ifconfig
</code></pre>

<p>In Terminal 2:</p>

<pre><code>#!sh
# execute a bash shell in your newly named network
ip netns exec ns1 bash

# check that you can see the guest veth device (down)
ifconfig -a

# bring the guest veth device up
# assign an ip and subnet to it
ip link set GUEST up
ip addr add 192.168.0.102/24

# check that the guest veth device is up with an address
ifconfig
</code></pre>

<p>Drumroooool &hellip;. In Terminal 2:</p>

<pre><code>#!sh
# check that you can ping the host veth device
# in the guest's network
ping 192.168.0.101
</code></pre>

<p>In Terminal 1:</p>

<pre><code>#!sh
# set up a netcat listening server
nc -l 192.168.0.101 5555
</code></pre>

<p>In Terminal 2:</p>

<pre><code>#!sh
# set up a netcat client
nc 192.168.0.101 5555

# say hi to the host veth device!
hi
</code></pre>

<p>You should see hi in Terminal 1. Mission accomplished.</p>

<p>&ldquo;Ugh, how do I tear this down?&rdquo; In Terminal 1:</p>

<pre><code>ip link delete host
</code></pre>

<p>If you check Terminal 2, you&rsquo;ll see that guest cannot live without it&rsquo;s buddy, and has already committed suicide.</p>

<p>To come:</p>

<ul>
<li>Attaching a veth device to the internet at large</li>
<li>Doing this all in a compiled language, for better portability</li>
</ul>

	</div>

    </div>
        <div class="preview">
        <div class="date">on Friday, July 12 &#39;13</div>
        <div class="title-home"><a href="./2013-07-12-Hash-collisions-for-the-skeptical.html">Hash collisions for the skeptical</a></div>

	<div class="post-wrapper">
	  <p>&ldquo;<strong>Gah</strong>&rdquo; you say.  &ldquo;I should probably be using something other than the dumbest way ever to resolve hash collisions in my super fast blah blah blah.&rdquo;  Or should you?  A good hash collision resolution strategy:</p>

<blockquote><p><em>Minimizes probing through your hash structure</em></p>

<p><em>Minimizes data cache misses</em></p></blockquote>

<p>Why do these matter?</p>

<p>Speed!  The more steps you take &mdash; the more indices you check &mdash; the farther your inserts and lookups get from O(1).  The farther apart your key&rsquo;s first hash location is from its new home in memory, the more cache lines you&rsquo;ll need to load into your data cache.  This takes time!  You&rsquo;re writing a super fast blah blah blah, remember?</p>

<p>&ldquo;Well, I just want to choose a strategy.&rdquo;  Alright, then let&rsquo;s have a method for doing so.  For each strategy:</p>

<ul>
<li><p>Insert and check the membership of <em>n</em> keys, where <em>n</em> is the size of the hash structure.  Output probe counts for both <code>insert</code> and <code>member</code>.  The lowest mean, median, and variance wins. <em>n</em>=4KB</p></li>
<li><p>Use <code>valgrind --tool=cachegrind</code> to profile cache miss rate with a virtual 32KB data cache.  The lowest cache miss rate wins. <em>n</em>=64KB, so the structure doesn&rsquo;t fit in the data cache.</p></li>
</ul>


<p>The strategies implemented:</p>

<p><em>Chaining.</em>  Each index in your hash structure holds a linked structure (list or tree).  Simply add your key to the end of the linked structure.  Dead simple. <a href="https://github.com/edahlgren/collisiontest/blob/master/chaining.c">code</a></p>

<p><em>Linear probing.</em>  Check the next index until you find an open place.  Iterate through each index (starting at the one your key hashes to) to find it again, iterating through all keys if it&rsquo;s not there.  Also really simple. <a href="https://github.com/edahlgren/collisiontest/blob/master/linear.c">code</a></p>

<p><em>Quadratic probing.</em>  Check the next <em>n</em>th index until you find a place.  <em>N</em> is determined by some quadratic function, so you&rsquo;ll hop around the hash structure in wider strides. <a href="https://github.com/edahlgren/collisiontest/blob/master/quadratic.c">code</a></p>

<p><em>Double hashing.</em>  Check the next <em>n</em>th index until you find a place.  <em>N</em> is determined by another hash function, so you&rsquo;ll hop around the hash structure in different strides per key. <a href="https://github.com/edahlgren/collisiontest/blob/master/double.c">code</a></p>

<p><em>Robin hood hashing.</em>  The idea is to evict keys from their place, and take their place, if you&rsquo;ve traveled farther away from your original hash index than they have from theirs.  Steal from the rich (the stupid bastards in your search path), and give to the poor (you who have traveled far). <a href="https://github.com/edahlgren/collisiontest/blob/master/robinhood.c">code</a></p>

<p><strong>Probe Count.</strong></p>

<div style="display:block; margin-top: 25px; margin-left:auto; margin-right:auto;">
<div style="display:block; float:left;">
Chaining
<table>
<tr><td></td><td style="color:#0000CC;">insert</td><td style="color:#0000CC;">member</td></tr>
<tr><td>mean</td><td>0.493</td><td>0.493</td></tr>
<tr><td>median</td><td>0.000</td><td>0.000</td></tr>
<tr><td>variance</td><td>0.565</td><td>0.565</td></tr>
<tr><td>deviation</td><td>0.751</td><td>0.751</td></tr>
<tr><td>max</td><td>4.000</td><td>4.000</td></tr>
</table>

Quadratic probing
<table>
<tr><td></td><td style="color:#0000CC;">insert</td><td style="color:#0000CC;">member</td></tr>
<tr><td>mean</td><td>6.891</td><td>6.891</td></tr>
<tr><td>median</td><td>1.000</td><td>1.000</td></tr>
<tr><td>variance</td><td>3191.905</td><td>3191.905</td></tr>
<tr><td>deviation</td><td>56.496</td><td>56.496</td></tr>
<tr><td>max</td><td>2982.000</td><td>2982.000</td></tr>
</table>

Robin hood hashing
<table>
<tr><td></td><td style="color:#0000CC;">insert</td><td style="color:#0000CC;">member</td></tr>
<tr><td>mean</td><td>1.554</td><td>30.752</td></tr>
<tr><td>median</td><td>1.000</td><td>27.000</td></tr>
<tr><td>variance</td><td>2.062</td><td>357.251</td></tr>
<tr><td>deviation</td><td>1.436</td><td>18.901</td></tr>
<tr><td>max</td><td>76.000</td><td>87.000</td></tr>
</table>
</div>

<div style="display:block;">
Linear probing
<table>
<tr><td></td><td style="color:#0000CC;">insert</td><td style="color:#0000CC;">member</td></tr>
<tr><td>mean</td><td>30.752</td><td>30.752</td></tr>
<tr><td>median</td><td>1.000</td><td>1.000</td></tr>
<tr><td>variance</td><td>27576.575</td><td>27576.575</td></tr>
<tr><td>deviation</td><td>166.061</td><td>166.061</td></tr>
<tr><td>max</td><td>3153.000</td><td>3153.000</td></tr>
</table>

Double hashing
<table>
<tr><td></td><td style="color:#0000CC;">insert</td><td style="color:#0000CC;">member</td></tr>
<tr><td>mean</td><td>6.663</td><td>6.663</td></tr>
<tr><td>median</td><td>1.000</td><td>1.000</td></tr>
<tr><td>variance</td><td>3450.241</td><td>3450.241</td></tr>
<tr><td>deviation</td><td>58.738</td><td>58.738</td></tr>
<tr><td>max</td><td>2014.000</td><td>2014.000</td></tr>
</table>
</div>
</div>




<div style="width:500px; display:inline-block;"></div>


<p>First notice that all strategies except for robin hood hashing have the same statistics for both <code>insert</code> and <code>member</code>.  Why?  In robing hood hashing, once you insert a key it will probably be evicted from that index one or times.  That means when you go to find it, you&rsquo;ll probably have to probe farther than you did to insert it.  <em>Any insert can induce many evictions.</em></p>

<p>In robin hood hashing this shows up as <code>member</code> performing considerably worse than <code>insert</code> (the median is 27 times greater, and the variance skyrockets!).  If your super fast blah blah blah is anything like a cache or ring, you&rsquo;ll be doing more reads (lookups) than writes (inserts).</p>

<p>But the sky turns grey when we look at linear probing, quadratic probing, and double hashing.  All three of these strategies are know to perform badly at >70% load [1]. We filled the hash structures up to 100%. Linear probing performs particularly ingloriously because keys with similar probe sequences clump: creating an uneven number of dangerious patches that cause lots of probing.  Quadratic probing and double hashing perform better because they take wider strides over the hash structure, effectively &ldquo;stepping over&rdquo; these key clumps.  While their mean probe counts are low (&lt; 7), the variance is still much higher than robin hood hashing.</p>

<p>How about chaining?  That dead simple strategy, remember?  Chaining gets a lot of heat for wasting space: After you <code>insert</code> <em>n</em> keys into your size <em>n</em> hash structure, you could very well have many indices totally empty.  There&rsquo;s no mechanism for distributing the tails of the linked list into the space you&rsquo;ve preallocated for the hash structure.  But do we really care?  The mean probe count is well under 1 and the variance is incredibly low.  Clearly the winner so far.</p>

<p><strong>Cache Misses.</strong></p>

<div style="margin-top:25px;">
Data cache
<table>
<tr><td></td><td>data refs</td><td>miss rate</td></tr>
<tr><td>chaining</td><td>109,316,426</td><td style="color:#0000CC;">0.3%</td</tr>
<tr><td>linear probing</td><td>265,519,800</td><td style="color:#0000CC;">3.8%</td></tr>
<tr><td>quadratic probing</td><td>127,420,471</td><td style="color:#0000CC;">1.9%</td></tr>
<tr><td>double hashing</td><td>128,662,471</td><td style="color:#0000CC;">2.1%</td></tr>
<tr><td>robin hood hashing</td><td>1,712,635,748</td><td style="color:#0000CC;">0.5%</td></tr>
</table>
</div>


<p>At first glance the cache miss rates look incorrect.  Linear probing, with its inorder search of our preallocated structure, should have the most cache line hits, right?  And if linear probing has the best cache line hit rate, then total cache misses should be lowest, right?</p>

<p>Well, if we weren&rsquo;t loading tons of cache lines into memory anyway, essentially if both our mean probe count and our probe count <em>variance</em> were low, yep you&rsquo;d be right.  But right now it seems that the cache benefits of the linear probing strategy are totally overwhelmed by high probe counts.  This could be made better if we were to fill our hash structure half full.  By why give chaining so much heat if you&rsquo;re now wasting 50% of your space?</p>

<p>Quadratic probing has a cache miss rate twice as small as linear probing.  This means that despite making wider jumps around the hash structure, meaning that hitting the same cache line twice in a row was less probable, the decreased probe count seems to have made a huge difference.  The same story can be said about double hashing, but less so because the wider jumps were less regular (the jump size varied by key).</p>

<p>Robin hood hashing and chaining performed marvelously, at less than 1% cache miss rate.  Theoretically robin hood hashing has better locality than chaining: If it takes very little probing to insert or lookup each element <em>stored in our preallocated hash structure</em>, then we&rsquo;re going to load few cache lines into our data cache.</p>

<p>In chaining, the only elements that are guaranteed good locality are the heads of the lists (stored in the preallocated hash structure).  And these aren&rsquo;t the elements we&rsquo;re probing!  We&rsquo;re probing through the elements of the linked lists, which are allocated individually.  How did chaining still beat robin hood hashing?</p>

<p>The answer: Remeber that with robin hood hashing, every insertion can induce many evictions?  Even though the mean probe count and variance are low, robin hood hashing still causes us to load more cache lines than chaining does simply because each insert often triggers a flood of other inserts, sometimes leading us far away from our starting index.</p>

<blockquote><p><em>Do you care about speed?</em><br>
Keep it simple, just use chaining.</p>

<p><em>Do you care about using your entire hash structure?</em><br>
Use chaining with a good hash function, but if you must, use robin hood hashing.</p>

<p>[1] <a href="http://www.it-c.dk/people/pagh/papers/linear.pdf">Linear Probing with Constant Independence</a><br>
<a href="http://en.wikipedia.org/wiki/Hash_table#Collision_resolution">Hash table: Collision resolution (wikipedia)</a><br>
<a href="http://stackoverflow.com/a/2349774/1672086">Choosing a quadratic probing function</a><br>
<a href="http://en.wikipedia.org/wiki/Double_hashing">Choosing a double hashing function</a><br>
<a href="http://sebastiansylvan.com/2013/05/08/robin-hood-hashing-should-be-your-default-hash-table-implementation">Robin hood hashing</a><br>
<a href="http://igoro.com/archive/gallery-of-processor-cache-effects/">Gallery of processor cache effects</a><br>
<a href="http://csqlcache.wordpress.com/2008/08/25/measuring-cache-misses/">Measuring cache misses</a></p></blockquote>
	</div>

    </div>
        <div class="preview">
        <div class="date">on Friday, July  5 &#39;13</div>
        <div class="title-home"><a href="./2013-07-05-Ring-balancing-for-the-weary.html">Ring balancing for the weary</a></div>

	<div class="post-wrapper">
	  <p>&ldquo;<strong>Ugh</strong>, Statistics!&rdquo; you say, &ldquo;I became a programmer in order to avoid this shit.&rdquo;  Me too, my friend, me too.  But there comes a time when you want to balance load between servers in a <a href="http://www.tom-e-white.com/2007/11/consistent-hashing.html">hash ring</a> and you end up wondering &ldquo;How many replicas of my servers do I need to be confident that my uniformly distributing hash function disperses those replicas evenly enough?&rdquo;  What you&rsquo;d like is simply a function that takes the size of your key space, the number of servers you want to store in the ring, and returns the number of replications per server to maintain a nearly uniform distribution.  Then you could move on with your life.</p>

<p>What does perfect look like?  If your servers hash to 32-bit integers, then your hash ring has 2<sup>32</sup> potential hashes.  Three servers uniformly distributed in a 32-bit space would be (2<sup>32</sup>)/3 32-bit integers apart.</p>

<p>&ldquo;When we add a new server, why don&rsquo;t we just move all of the existing servers along the ring a little to make room for the new one?&rdquo;  Sorry.  The point of using a consistent hash ring is to be able to remap as few data keys to new servers as possible when a server is added or deleted.  If we changed the hash position of every server in the ring, we&rsquo;d likely be remapping a lot of data keys to new servers.</p>

<blockquote><p><em>Constraint.</em> Move as few servers on the hash ring as possible.</p></blockquote>

<p><strong>Lazy.</strong>  What&rsquo;s the easiest way to be certain you have a uniform distribution over a space?  Fill ALL the spaces!  Imagine you&rsquo;re using a 16-bit hash space.  This isn&rsquo;t crazy, since you&rsquo;re only storing your servers in your hash ring usually.  A 16-bit hash space means 2<sup>16</sup> or 65536 potential hashes.  If you have no more than 100 servers, you could &ldquo;replicate&rdquo; &mdash; hash unique keys with your server name as the prefix &mdash; each server 650 times so that you would store 65500 keys in your ring.</p>

<blockquote><p><em>Collisions?</em>  Just rehash with an incrementing postfix until you find a hash that isn&rsquo;t taken.</p>

<p><em>Hash function?</em>  Use a fast one that produces uniformly distributed hashes and has good avalanche (collision resistance) behavior like the <a href="http://home.comcast.net/~bretm/hash/7.html">Jenkin&rsquo;s hash</a>.</p></blockquote>

<p>How uniform is this?  In the worst case, all 65500 are clumped together with a 65536-65500=36 gap.  That means 36/65536 of your ring is not uniformly distributed, only 0.05%.  Not bad.</p>

<p>What happens if you lose a machine?  You go from 100 machines to 99 machines, so you mark the 650 hashes in your ring corresponding to the dead sucker for removal.  Now you&rsquo;ve gone from 65500 hashes to 65500-650=64850.  Fewer machines means that you can have more replicas per machine.  If we round down 65536/99 we get 661 replications.  So we add 661-650=6 extra replicas per machine into the ring.  We&rsquo;re closer to 65536 now, with 65439 hashes in the ring.</p>

<p>What did that do to the likelihood that the ring is still just as uniformly distributed?  Worst case we have gap of size 65536-65439=97, so 97/65536 of the ring is not uniformly distributed, still very low at 0.14%.  As we approach zero machines, the likelihood that we&rsquo;ll be able to evenly divide our machine count into 65536 increases (perfect uniformity!), so the ring will stay nonuniform in the 0-1% range.  Also not bad.</p>

<p>&ldquo;Wait, I have to keep track of up to 65536 ordered hashes?&rdquo;  Yep.  You have to store all of those hashes and their metadata in memory somewhere.  For 65536 16-bit hashes, this isn&rsquo;t a big deal (around a megabyte with a string of IP metadata).  What if you&rsquo;re using 32-bit hashes, because you&rsquo;ve got significantly more than 100 machines?  That&rsquo;s 4294967296 32-bit hashes with metadata (around 50 gigabytes), which most modern laptops cannot fit in RAM.  If you&rsquo;re using a sorted data structure (useful for finding the &ldquo;next&rdquo; machine in the ring for a given data key), that means O(log n) for lookups or insertions.  That could also be quite annoying.</p>

<p><strong>Hopeful.</strong>  What if we were to find the largest gap in our ring and put our next hash right in the middle of this gap?  Throw the hash function out the window!  This is what would happen:</p>

<p><img src="./static/images/LargestGap.jpg" alt="Uniform" style="width: 300px;display:block; margin-left:auto; margin-right:auto;"/></p>

<p>As we add servers, the ring is only balanced when we have 2<sup>n</sup> servers, at 2,4,8,16,etc.  We&rsquo;ll never be able to have a perfectly balanced ring with three servers, for instance, or any odd number for that matter.  Removing servers is also a pain, because it&rsquo;s not intuitive how we would rebalance.  This is a hopeful approach, but don&rsquo;t take it.</p>

<p><strong>Empirical.</strong>  We can still hash servers into the ring, and fewer of them than the entire ring, if we know what degree of nonuniformity we can handle.  Empirically,</p>

<ol>
<li>Hash r replications of n servers into a s-bit space</li>
<li>Vary r by 1,10,100,500,1000,5000,10000 replications, so long as n*r is less than the number of hashes the space can fit</li>
<li>Vary s by 3,10,100,500,1000 servers, so long as n*r is also less than the size of the space</li>
<li>For each replica in the ring, find the number of data keys that would get routed to it</li>
<li>Sum up the number of data keys that would go to each server by reducing the replicas back to single servers</li>
<li>Get the difference between the number of data keys that would go to each server in a uniform world (2<sup>s</sup>)/n and the actual number of data keys per server</li>
<li>The sum of the differences in 6 gives us the nonuniformity of the entire ring</li>
</ol>


<p>Here are the results for a 16-bit space and 32-bit space (<a href="https://gist.github.com/edahlgren/5938401">code</a>):</p>

<div style="margin-left:auto; margin-right:auto;">
<div style="display:block; overflow:hidden; float:left; margin-left:25px; margin-right:50px;">
<div style="text-align:center;">16-bit</div>
<table>
<tr>
<td>servers</td>
<td>replicas</td>
<td>% nonuniform</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>30.415</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>19.422</td>
</tr>
<tr>
<td>3</td>
<td>100</td>
<td>6.370</td>
</tr>
<tr>
<td>3</td>
<td>500</td>
<td style="color:#0000CC;">0.724</td>
</tr>
<tr>
<td>3</td>
<td>1000</td>
<td>1.295</td>
</tr>
<tr>
<td>3</td>
<td>5000</td>
<td style="color:#0000CC;">0.593</td>
</tr>
<tr>
<td>3</td>
<td>10000</td>
<td style="color:#0000CC;">0.331</td>
</tr>
<tr></tr>
<tr>
<td>10</td>
<td>1</td>
<td>82.101</td>
</tr>
<tr>
<td>10</td>
<td>10</td>
<td>35.485</td>
</tr>
<tr>
<td>10</td>
<td>100</td>
<td>5.508</td>
</tr>
<tr>
<td>10</td>
<td>500</td>
<td>2.316</td>
</tr>
<tr>
<td>10</td>
<td>1000</td>
<td>2.249</td>
</tr>
<tr>
<td>10</td>
<td>5000</td>
<td style="color:#0000CC;">0.531</td>
</tr>
<tr></tr>
<tr>
<td>100</td>
<td>1</td>
<td>74.716</td>
</tr>
<tr>
<td>100</td>
<td>10</td>
<td>26.733</td>
</tr>
<tr>
<td>100</td>
<td>100</td>
<td>6.497</td>
</tr>
<tr>
<td>100</td>
<td>500</td>
<td>1.815</td>
</tr>
<tr></tr>
<tr>
<td>500</td>
<td>1</td>
<td>75.839</td>
</tr>
<tr>
<td>500</td>
<td>10</td>
<td>22.915</td>
</tr>
<tr>
<td>500</td>
<td>100</td>
<td>3.948</td>
</tr>
<tr></tr>
<tr>
<td>1000</td>
<td>1</td>
<td>70.220</td>
</tr>
<tr>
<td>1000</td>
<td>10</td>
<td>22.842</td>
</tr>
</table>
</div>

<div style="display:block; overflow:hidden; margin-left:50px; margin-right:auto;">
<div style="text-align:center;">32-bit</div>
<table>
<tr>
<td>servers</td>
<td>replicas</td>
<td>% nonuniform</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>30.415</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>19.418</td>
</tr>
<tr>
<td>3</td>
<td>100</td>
<td>6.379</td>
</tr>
<tr>
<td>3</td>
<td>500</td>
<td style="color:#0000CC;">0.660</td>
</tr>
<tr>
<td>3</td>
<td>1000</td>
<td>1.731</td>
</tr>
<tr>
<td>3</td>
<td>5000</td>
<td style="color:#0000CC;">0.647</td>
</tr>
<tr>
<td>3</td>
<td>10000</td>
<td style="color:#0000CC;">0.333</td>
</tr>
<tr></tr>
<tr>
<td>10</td>
<td>1</td>
<td>82.105</td>
</tr>
<tr>
<td>10</td>
<td>10</td>
<td>35.483</td>
</tr>
<tr>
<td>10</td>
<td>100</td>
<td>5.733</td>
</tr>
<tr>
<td>10</td>
<td>500</td>
<td>2.186</td>
</tr>
<tr>
<td>10</td>
<td>1000</td>
<td>2.476</td>
</tr>
<tr>
<td>10</td>
<td>5000</td>
<td>1.624</td>
</tr>
<tr>
<td>10</td>
<td>10000</td>
<td style="color:#0000CC;">0.694</td>
</tr>
<tr></tr>
<tr>
<td>100</td>
<td>1</td>
<td>74.740</td>
</tr>
<tr>
<td>100</td>
<td>10</td>
<td>27.578</td>
</tr>
<tr>
<td>100</td>
<td>100</td>
<td>7.369</td>
</tr>
<tr>
<td>100</td>
<td>500</td>
<td>2.938</td>
</tr>
<tr>
<td>100</td>
<td>1000</td>
<td>2.314</td>
</tr>
<tr>
<td>100</td>
<td>5000</td>
<td>1.233</td>
</tr>
<tr>
<td>100</td>
<td>10000</td>
<td style="color:#0000CC;">0.781</td>
</tr>
<tr></tr>
<tr>
<td>500</td>
<td>1</td>
<td>75.939</td>
</tr>
<tr>
<td>500</td>
<td>10</td>
<td>23.786</td>
</tr>
<tr>
<td>500</td>
<td>100</td>
<td>7.856</td>
</tr>
<tr>
<td>500</td>
<td>500</td>
<td>3.711</td>
</tr>
<tr>
<td>500</td>
<td>1000</td>
<td>2.598</td>
</tr>
</table>
</div>
</div>


<p>It was surprising to find that even 500 replicas of 3 servers in a 16-bit and 32-bit space could so readily result in a nearly (99%) uniform ring.  This test was only run once and there needs to be averaging over several runs, but the take-home is that you need far fewer keys than the full hash space to produce uniform rings at 99% uniformity.  Also more replicas with few servers is much better than more servers and few replicas.</p>

<blockquote><p><em>Ok, so how many replicas of my servers do I need?</em></p>

<p>Simple: Choose the smallest bit-space that supports at least 500 replications per server.<br></p>

<p>Better: Run the tests!  (and make the tests better!)</p></blockquote>
	</div>

    </div>
        <div class="preview">
        <div class="date">on Wednesday, July  3 &#39;13</div>
        <div class="title-home"><a href="./2013-07-03-Hash-functions-for-the-weary.html">Hash functions for the weary</a></div>

	<div class="post-wrapper">
	  <p>&ldquo;<strong>Damnit</strong>&rdquo;, you say.  &ldquo;Building this hash ring or routing this data means that I need to find the fastest hash function that isn&rsquo;t horrible.&rdquo;  The reason you&rsquo;ve said &ldquo;Damnit!&rdquo; is because what makes a hash function horrible is knowledge you&rsquo;ve popped off your memory stack long ago.  Simply put, a non-horrible hash function:</p>

<blockquote><p>Distributes your keys randomly (evenly) across your key space</p>

<p>Avoids collisions with other keys</p>

<p>Maybe is irreversible</p>

<p>Maybe compresses your key into a checksum</p>

<p>Is quick!</p></blockquote>

<p>What do you care about?  Maybe you care that all your machines have the same load, so you care that your keys hash to a uniform distribution.  Collisions don&rsquo;t matter to you because you&rsquo;re not storing anything about keys that hash to the same place.  Maybe you want to avoid iterating through a data structure when multiple values are stored at the same place, so you care about minimizing key collisions.  Maybe you care about garaunteeing that two messages aren&rsquo;t the same very quickly.  Maybe you care about how easy it would be to brute force find a collision for your hash.  Irreversable output (160 bit large) matters most to you.  But always, you care about &ldquo;Moar speed!&rdquo;</p>

<p>It&rsquo;s easy to evaulate a hash function for speed (clock those cpu seconds), or output size (derp de derp, how many bytes is this thing?).  Less obvious, a hash function produces a uniform distribution if:</p>

<blockquote><p>Each pattern of output bits is equally likely</p></blockquote>

<p>A hash function avoids collisions when:</p>

<blockquote><p>Changing a single input bit drastically affects output bits</p></blockquote>

<p>When you hear people talking about <em>avalanche</em>, this is what they mean.  Formally, &ldquo;drastically&rdquo; means that if an input bit changes, each output bit should change with a 50% probability.  When an input bit has a 0% or 100% affect on an output bit, we say that the output bit isn&rsquo;t <em>mixed</em>.  If this happens to a lot of your bits, then your hashing function <em>mixes poorly</em>.  Your hash function will also avoid collisions if:</p>

<blockquote><p>Output bits are independent of one another</p></blockquote>

<p>Those are the three properities you&rsquo;re looking for if you want your hash function to produce uniformly distributed and collision resistant output.  Now that we know what we&rsquo;re looking for and what we care about, let&rsquo;s look at some options:</p>

<p><strong>CRC32.</strong>  Standing for cyclic redundancy check, this simply produces a 32-bit &ldquo;tl;dr&rdquo; of your message (a <a href="http://en.wikipedia.org/wiki/Checksum">checksum</a>), useful to checking whether your message was transmitted with errors.  It&rsquo;s computed by continuously dividing a smaller and smaller slices of the input by a polynomial, eventually resulting in a 32-bit remainder [1].  <em>Pros.</em>  Since your polynomial is usually fixed, one can write very fast CRC implementations involving pre-computed lookup tables.  <em>Cons.</em>  A chi-squared test [2] shows that only half of the CRC32&rsquo;s output bits were uniformly distributed; each input bit affects each output bit at less than 33% probability or greater than 66% probability (we want 50%), meaning that it has abysmal collision resistance [2].</p>

<p><strong>FNV.</strong> Standing for its creators: (Glenn) Fowler/(Landon Cur) Noll/(Phong) Vo, FNV is a very simple hash function.  The idea is to take a starting seed, and continually multiply it by a specific prime and XOR it with the next byte of your input.  <em>Pros.</em>  The simplicity of the algorithm makes it fast enough to be used as a checksum.  <em>Cons.</em>  Its simplicity makes it trivial to brute force find collisions.  A chi-sqaured test shows that output bits are uniformly distributed up to only 2<sup>14</sup> [3].  It also appears that the last byte of the input does not cause any mixing at all [3], meaning that this function has poor collision resistance.</p>

<p><strong>SIPHASH.</strong> The hipster on the block, SipHash is a relatively new hash function (2012) which boasts to have better collision resistance than FNV (not unimaginable) and to be just as fast or faster. There are few benchmarks available, some of them only comparing Cycles/Byte (and only between SipHash and several <a href="https://en.wikipedia.org/wiki/Hash-based_message_authentication_code">hmac</a> implementations) [4], or language internal improvements over FNV [5].  I don&rsquo;t have enough information to recommend this function in terms of uniform distribution or collision resistance, but its speed looks comparable to FNV.</p>

<p><strong>JENKINS.</strong> Named after its creator Bob Jenkins, this hash function mixes keys 12 bytes at a time [6].  The mixing is a complicated sequence of shifts, adds, and XORs, making it non-trivial to implement.  <em>Pros.</em>  It produces uniformly distributed output, and has excellent collision resistance due to good mixing (every input bit causes a change in every output bit at 33-66% probability) [7].  Since the mixing can be done on parallel processors, it is still relatively fast.  <em>Cons.</em> It is not cryptographically secure and it is easy to implement incorrectly.</p>

<p><strong>SHA-1.</strong> Standing for Secure Hash Algorithm, this hash function produces cryptographic (hopefully irreversable) 160-bit output.  We can compare SHA-1 to our other algorithms (which can produce 32-bit output) by XOR-ing the five 32-bit sections of the 160-bit output together.  <em>Pros.</em>  Cryptographic hashes need to be uniformly distributed and they need to mix well, or else finding collisions by brute force would be possible.  SHA-1 has good mixing like Jenkins and produces output that is equally uniformly distributed [8].  It is also mostly cryptographically secure [9].  <em>Cons.</em>  Computing a 160-bit hash takes cpu seconds any way you slice it, so it&rsquo;s totally unnecessary if security isn&rsquo;t required.</p>

<p>Know what you want your hash function to do, and what you can compromise to get there!</p>

<blockquote><p>[1] <a href="http://en.wikipedia.org/wiki/Cyclic_redundancy_check">Cylic redundancy check: wikipedia</a><br>
[2] <a href="http://home.comcast.net/~bretm/hash/8.html">CRC32 test: Bret Mulvey</a><br>
[3] <a href="http://home.comcast.net/~bretm/hash/6.html">FNV test: Bret Mulvey</a><br>
[4] <a href="http://bench.cr.yp.to/results-auth.html">SipHash Bytes/Cycle benchmarks</a><br>
[5] <a href="http://www.serpentine.com/blog/2012/10/02/a-fast-new-siphash-implementation-in-haskell/">SipHash Haskell benchmarks</a><br>
[6] <a href="http://en.wikipedia.org/wiki/Jenkins_hash_function">Jenkins hash function: wikipedia</a><br>
[7] <a href="http://home.comcast.net/~bretm/hash/7.html">Jenkins test: Bret Mulvey</a><br>
[8] <a href="http://home.comcast.net/~bretm/hash/9.html">SHA-1 test: Bret Mulvey</a><br>
[9] <a href="http://www.schneier.com/blog/archives/2005/02/cryptanalysis_o.html">Bruce Schneier on SHA-1</a></p></blockquote>
	</div>

    </div>
        <div class="preview">
        <div class="date">on Sunday, June 23 &#39;13</div>
        <div class="title-home"><a href="./2013-06-23-Hobogen.html">Hobogen</a></div>

	<div class="post-wrapper">
	  <p><a href="http://github.com/edahlgren/hobo">Hobogen</a> is an html compiler written in haskell.  It is an extension of the minimalist blogging engine <a href="http://github.com/jamwt/hobo">hobo</a>.</p>

<p>Futex 0x3be6fec is generated by hobogen. I&rsquo;ve used it also to generate my <a href="prose.edahlgren.com">prose writings collection</a>.  Based on a set of <a href="http://mustache.github.io/">mustache</a> tags that you include in your html templates, and a posts that you write in simple (markdown)[http://whatismarkdown.com/], hobogen produces static html for:</p>

<ul>
<li>A home page, with a full preview of the post (like here), or an n-line preview of the post (like on the prose site)</li>
<li>A page for each of your posts</li>
<li>An archive page, where your posts are grouped by month</li>
<li>An about page where you can describe your site</li>
</ul>


<p>Hobogen builds custom headers and footers into generated html using head.html and foot.html templates.  These are good places to import css and javascript styles specific to your site.  Hobogen inherits from hobo the ability to highlight code blocks specific to a particular language. Here is a sample of the hobogen code that compiles the archive page:</p>

<pre><code>#!haskell
compileArchive ts cfg = do
    rpaths &lt;- getDirectoryContents (base cfg &lt;/&gt; "posts")
    let paths = (reverse . sort) rpaths
    let archives = catMaybes $ map parseArchive paths
    let groupedArchives =
      L.groupBy (\(y1,m1,_,_) (y2,m2,_,_) -&gt; y1 == y2 &amp;&amp; m1 == m2) archives
    (archiveTemplate ts) (sub groupedArchives)
</code></pre>

<p>In that code block, hobogen is reading all of the posts paths, sorting them in ascending dated order, parsing the parts of the posts for valid year, month, title and url data needed to construct the archive page (using the <a href="http://hackage.haskell.org/package/attoparsec-0.10.4.0">Attoparsec</a> parser), grouping the posts written in the same year and month, and building the html from the archive template.</p>

<p>If you&rsquo;d like to either to use or contribute to hobogen, contact <a href="https://github.com/edahlgren">me</a> through github.</p>
	</div>

    </div>
        </div>
</div>
</body>
</html>
